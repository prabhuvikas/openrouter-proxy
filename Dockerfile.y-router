# syntax=docker/dockerfile:1
# Standalone Node.js proxy for Anthropic to OpenAI format conversion
# Properly handles Claude Code requests and forwards to OpenRouter

FROM node:22-bookworm-slim

# Install curl for health checks
RUN apt-get update && apt-get install -y --no-install-recommends curl && rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Create package.json
COPY <<EOF package.json
{
  "name": "openrouter-proxy",
  "version": "1.0.0",
  "description": "Anthropic to OpenRouter proxy",
  "main": "proxy.js",
  "scripts": {
    "start": "node proxy.js"
  },
  "dependencies": {
    "express": "^4.18.2"
  }
}
EOF

# Install dependencies
RUN npm install

# Create proxy server that converts Anthropic API to OpenAI format
COPY <<'PROXYEOF' proxy.js
const express = require("express");
const https = require("https");

const app = express();
app.use(express.json({ limit: "50mb" }));

const PORT = 8787;
const OPENROUTER_BASE = "https://openrouter.ai/api/v1";

// Retry and fallback configuration
const FALLBACK_MODEL = process.env.PROXY_MODEL_FALLBACK || 'z-ai/glm-4.5-air';
const MAX_RETRIES = parseInt(process.env.PROXY_MAX_RETRIES || '3', 10);
const RETRY_DELAY_MS = parseInt(process.env.PROXY_RETRY_DELAY_MS || '1000', 10);
const FALLBACK_ON_RATE_LIMIT = process.env.PROXY_FALLBACK_ON_RATE_LIMIT !== 'false';

// ============================================================================
// USAGE STATISTICS (in-memory, resets on restart)
// ============================================================================

const stats = {
  startTime: Date.now(),
  requests: {
    total: 0,
    streaming: 0,
    nonStreaming: 0,
    withTools: 0
  },
  tokens: {
    input: 0,
    output: 0
  },
  models: {},        // { "model-name": { requests: N, inputTokens: N, outputTokens: N } }
  errors: {
    total: 0,
    rateLimits: 0,
    apiErrors: 0,
    networkErrors: 0
  },
  fallbacks: 0,      // Times fallback model was used
  lastRequest: null  // Timestamp of last request
};

function recordRequest(model, isStreaming, hasTools) {
  stats.requests.total++;
  stats.lastRequest = Date.now();

  if (isStreaming) stats.requests.streaming++;
  else stats.requests.nonStreaming++;

  if (hasTools) stats.requests.withTools++;

  if (!stats.models[model]) {
    stats.models[model] = { requests: 0, inputTokens: 0, outputTokens: 0 };
  }
  stats.models[model].requests++;
}

function recordTokens(model, inputTokens, outputTokens) {
  stats.tokens.input += inputTokens;
  stats.tokens.output += outputTokens;

  if (stats.models[model]) {
    stats.models[model].inputTokens += inputTokens;
    stats.models[model].outputTokens += outputTokens;
  }
}

function recordError(type) {
  stats.errors.total++;
  if (type === 'rate_limit') stats.errors.rateLimits++;
  else if (type === 'api_error') stats.errors.apiErrors++;
  else if (type === 'network_error') stats.errors.networkErrors++;
}

function recordFallback() {
  stats.fallbacks++;
}

function formatUptime(ms) {
  const seconds = Math.floor(ms / 1000);
  const minutes = Math.floor(seconds / 60);
  const hours = Math.floor(minutes / 60);
  const days = Math.floor(hours / 24);

  if (days > 0) return `${days}d ${hours % 24}h ${minutes % 60}m`;
  if (hours > 0) return `${hours}h ${minutes % 60}m ${seconds % 60}s`;
  if (minutes > 0) return `${minutes}m ${seconds % 60}s`;
  return `${seconds}s`;
}

// ============================================================================
// FORMAT CONVERSION FUNCTIONS
// ============================================================================

// Convert Anthropic tool_choice to OpenAI format
// Anthropic: { type: "auto" | "any" | "tool", name?: string }
// OpenAI: "auto" | "required" | { type: "function", function: { name: string } }
function convertToolChoice(anthropicChoice) {
  if (!anthropicChoice) return undefined;

  if (anthropicChoice.type === 'auto') return 'auto';
  if (anthropicChoice.type === 'any') return 'required';
  if (anthropicChoice.type === 'tool' && anthropicChoice.name) {
    return { type: 'function', function: { name: anthropicChoice.name } };
  }

  // Pass through if already in OpenAI format or unknown
  return anthropicChoice;
}

// Convert Anthropic tools to OpenAI functions
function convertTools(anthropicTools) {
  if (!anthropicTools || !Array.isArray(anthropicTools)) return undefined;

  return anthropicTools.map(tool => ({
    type: 'function',
    function: {
      name: tool.name,
      description: tool.description || '',
      parameters: tool.input_schema || { type: 'object', properties: {} }
    }
  }));
}

// Convert Anthropic messages to OpenAI format (with agent/MCP support)
// Handles: text, tool_use (in assistant messages), tool_result (in user messages)
function convertMessages(anthropicMessages) {
  const openaiMessages = [];

  for (const msg of anthropicMessages) {
    // Simple string content
    if (typeof msg.content === 'string') {
      openaiMessages.push({
        role: msg.role,
        content: msg.content
      });
      continue;
    }

    // Array content - needs special handling
    if (Array.isArray(msg.content)) {
      if (msg.role === 'assistant') {
        // Assistant messages may contain tool_use blocks
        const textParts = [];
        const toolCalls = [];

        for (const block of msg.content) {
          if (block.type === 'text') {
            textParts.push(block.text);
          } else if (block.type === 'tool_use') {
            // Convert Anthropic tool_use to OpenAI tool_calls format
            toolCalls.push({
              id: block.id,
              type: 'function',
              function: {
                name: block.name,
                arguments: typeof block.input === 'string'
                  ? block.input
                  : JSON.stringify(block.input || {})
              }
            });
          }
        }

        const assistantMsg = {
          role: 'assistant',
          content: textParts.join('\n') || null
        };

        if (toolCalls.length > 0) {
          assistantMsg.tool_calls = toolCalls;
        }

        openaiMessages.push(assistantMsg);
      } else if (msg.role === 'user') {
        // User messages may contain tool_result blocks
        // These need to be split into separate "tool" role messages
        const textParts = [];
        const toolResults = [];

        for (const block of msg.content) {
          if (block.type === 'text') {
            textParts.push(block.text);
          } else if (block.type === 'tool_result') {
            toolResults.push(block);
          } else if (block.type === 'image') {
            // Handle image blocks (vision)
            textParts.push({
              type: 'image_url',
              image_url: {
                url: block.source?.data
                  ? `data:${block.source.media_type};base64,${block.source.data}`
                  : block.source?.url || ''
              }
            });
          } else {
            // Pass through other content types
            textParts.push(block);
          }
        }

        // Add tool results as separate "tool" role messages (OpenAI format)
        for (const tr of toolResults) {
          let content = tr.content;

          // Handle array content in tool results
          if (Array.isArray(content)) {
            content = content.map(c => {
              if (typeof c === 'string') return c;
              if (c.type === 'text') return c.text;
              return JSON.stringify(c);
            }).join('\n');
          } else if (typeof content !== 'string') {
            content = JSON.stringify(content);
          }

          openaiMessages.push({
            role: 'tool',
            tool_call_id: tr.tool_use_id,
            content: content || ''
          });
        }

        // Add text content as user message if present
        if (textParts.length > 0) {
          const hasComplexContent = textParts.some(p => typeof p !== 'string');
          openaiMessages.push({
            role: 'user',
            content: hasComplexContent ? textParts : textParts.join('\n')
          });
        }
      } else {
        // Other roles - pass through
        openaiMessages.push({
          role: msg.role,
          content: msg.content
        });
      }
    } else {
      // Fallback - pass through as-is
      openaiMessages.push({
        role: msg.role,
        content: msg.content
      });
    }
  }

  return openaiMessages;
}

// Main request converter: Anthropic -> OpenAI
function anthropicToOpenAI(anthropicMsg) {
  let messages = convertMessages(anthropicMsg.messages || []);

  // Handle system prompt - insert as first message
  if (anthropicMsg.system) {
    const systemContent = typeof anthropicMsg.system === 'string'
      ? anthropicMsg.system
      : Array.isArray(anthropicMsg.system)
        ? anthropicMsg.system.map(s => s.text || s).join('\n')
        : anthropicMsg.system;

    messages.unshift({
      role: 'system',
      content: systemContent
    });
  }

  const openaiMsg = {
    model: anthropicMsg.model,
    messages: messages,
    max_tokens: anthropicMsg.max_tokens,
    temperature: anthropicMsg.temperature,
    top_p: anthropicMsg.top_p,
    stream: anthropicMsg.stream === true
  };

  // Add tools if present
  if (anthropicMsg.tools && anthropicMsg.tools.length > 0) {
    openaiMsg.tools = convertTools(anthropicMsg.tools);
  }

  // Convert tool_choice
  if (anthropicMsg.tool_choice) {
    openaiMsg.tool_choice = convertToolChoice(anthropicMsg.tool_choice);
  }

  // Convert stop_sequences to stop
  if (anthropicMsg.stop_sequences) {
    openaiMsg.stop = anthropicMsg.stop_sequences;
  }

  // Remove undefined values
  Object.keys(openaiMsg).forEach(key => openaiMsg[key] === undefined && delete openaiMsg[key]);

  console.log("Converted request - tools:", openaiMsg.tools ? openaiMsg.tools.length : 0,
              "tool_choice:", openaiMsg.tool_choice,
              "messages:", openaiMsg.messages.length,
              "stream:", openaiMsg.stream);
  return openaiMsg;
}

// ============================================================================
// NON-STREAMING RESPONSE CONVERSION
// ============================================================================

// Convert OpenAI complete response to Anthropic format
function openAIResponseToAnthropic(openaiResponse, model) {
  if (!openaiResponse.choices || openaiResponse.choices.length === 0) {
    return {
      id: openaiResponse.id || 'msg_' + Date.now(),
      type: 'message',
      role: 'assistant',
      content: [{ type: 'text', text: '' }],
      model: model,
      stop_reason: 'end_turn',
      usage: {
        input_tokens: openaiResponse.usage?.prompt_tokens || 0,
        output_tokens: openaiResponse.usage?.completion_tokens || 0
      }
    };
  }

  const choice = openaiResponse.choices[0];
  const message = choice.message || {};

  // Build content array
  const content = [];

  // Add text content if present
  if (message.content && typeof message.content === 'string' && message.content.trim()) {
    content.push({
      type: 'text',
      text: message.content
    });
  }

  // Convert tool calls to Anthropic tool_use format
  if (message.tool_calls && Array.isArray(message.tool_calls)) {
    console.log("Converting", message.tool_calls.length, "tool calls to Anthropic format");

    for (const toolCall of message.tool_calls) {
      if (toolCall.type === 'function') {
        let parsedInput = {};

        if (typeof toolCall.function.arguments === 'string') {
          try {
            parsedInput = JSON.parse(toolCall.function.arguments);
          } catch (e) {
            console.error("Failed to parse tool arguments:", e.message);
            // Try to salvage what we can
            parsedInput = { _raw: toolCall.function.arguments };
          }
        } else {
          parsedInput = toolCall.function.arguments || {};
        }

        content.push({
          type: 'tool_use',
          id: toolCall.id,
          name: toolCall.function.name,
          input: parsedInput
        });
      }
    }
  }

  // Ensure at least empty text if no content
  if (content.length === 0) {
    content.push({
      type: 'text',
      text: ''
    });
  }

  // Determine stop_reason
  let stopReason = 'end_turn';
  if (message.tool_calls && message.tool_calls.length > 0) {
    stopReason = 'tool_use';
  } else if (choice.finish_reason === 'length') {
    stopReason = 'max_tokens';
  } else if (choice.finish_reason === 'stop') {
    stopReason = 'end_turn';
  } else if (choice.finish_reason === 'tool_calls') {
    stopReason = 'tool_use';
  }

  return {
    id: openaiResponse.id || 'msg_' + Date.now(),
    type: 'message',
    role: 'assistant',
    content: content,
    model: model,
    stop_reason: stopReason,
    usage: {
      input_tokens: openaiResponse.usage?.prompt_tokens || 0,
      output_tokens: openaiResponse.usage?.completion_tokens || 0
    }
  };
}

// ============================================================================
// STREAMING RESPONSE CONVERSION
// ============================================================================

// Create a new stream state for tracking streaming responses
function createStreamState(model) {
  return {
    model: model,
    messageId: 'msg_' + Date.now(),
    started: false,           // Has message_start been sent?
    textBlockStarted: false,  // Has text content_block_start been sent?
    textContent: '',          // Accumulated text content
    toolCalls: {},            // Track tool calls by index: { id, name, args }
    toolBlocksStarted: {},    // Track which tool blocks have been started
    contentIndex: 0,          // Current content block index
    finishReason: null        // Final finish reason
  };
}

// Process a streaming chunk and return Anthropic SSE events
function processStreamingChunk(line, streamState) {
  const events = [];

  // Parse SSE data line
  if (!line.startsWith('data: ')) {
    return events;
  }

  const jsonStr = line.slice(6).trim();
  if (jsonStr === '[DONE]') {
    return events; // Will be handled in stream end
  }

  let data;
  try {
    data = JSON.parse(jsonStr);
  } catch (e) {
    console.error('Failed to parse streaming chunk:', e.message);
    return events;
  }

  if (!data.choices || !data.choices[0]) {
    return events;
  }

  const choice = data.choices[0];
  const delta = choice.delta || {};

  // Emit message_start on first chunk
  if (!streamState.started) {
    streamState.started = true;
    events.push({
      type: 'message_start',
      message: {
        id: streamState.messageId,
        type: 'message',
        role: 'assistant',
        content: [],
        model: streamState.model,
        stop_reason: null,
        stop_sequence: null,
        usage: { input_tokens: 0, output_tokens: 0 }
      }
    });
  }

  // Handle text content delta
  if (delta.content) {
    // Start text block if not started
    if (!streamState.textBlockStarted) {
      streamState.textBlockStarted = true;
      streamState.contentIndex = 0;
      events.push({
        type: 'content_block_start',
        index: 0,
        content_block: { type: 'text', text: '' }
      });
    }

    // Send text delta
    streamState.textContent += delta.content;
    events.push({
      type: 'content_block_delta',
      index: 0,
      delta: { type: 'text_delta', text: delta.content }
    });
  }

  // Handle tool calls delta
  if (delta.tool_calls && delta.tool_calls.length > 0) {
    for (const tc of delta.tool_calls) {
      const idx = tc.index;

      // Initialize tool call tracking if new
      if (!streamState.toolCalls[idx]) {
        streamState.toolCalls[idx] = { id: '', name: '', args: '' };
      }

      const toolState = streamState.toolCalls[idx];

      // Capture tool call id
      if (tc.id) {
        toolState.id = tc.id;
      }

      // Capture/accumulate function name
      if (tc.function?.name) {
        toolState.name += tc.function.name;
      }

      // Accumulate function arguments
      if (tc.function?.arguments) {
        toolState.args += tc.function.arguments;
      }

      // Start tool_use block when we have id and name
      if (toolState.id && toolState.name && !streamState.toolBlocksStarted[idx]) {
        // Close text block first if it was started
        if (streamState.textBlockStarted && streamState.contentIndex === 0) {
          events.push({
            type: 'content_block_stop',
            index: 0
          });
          streamState.contentIndex = 1;
        } else if (!streamState.textBlockStarted) {
          // No text block was started, tool starts at index 0
          streamState.contentIndex = 0;
        }

        streamState.toolBlocksStarted[idx] = true;
        const toolIndex = streamState.textBlockStarted ? idx + 1 : idx;

        events.push({
          type: 'content_block_start',
          index: toolIndex,
          content_block: {
            type: 'tool_use',
            id: toolState.id,
            name: toolState.name,
            input: {}
          }
        });
      }

      // Send input_json_delta for arguments
      if (tc.function?.arguments && streamState.toolBlocksStarted[idx]) {
        const toolIndex = streamState.textBlockStarted ? idx + 1 : idx;
        events.push({
          type: 'content_block_delta',
          index: toolIndex,
          delta: {
            type: 'input_json_delta',
            partial_json: tc.function.arguments
          }
        });
      }
    }
  }

  // Handle finish reason
  if (choice.finish_reason) {
    streamState.finishReason = choice.finish_reason;
  }

  return events;
}

// Generate final events when stream ends
function generateStreamEndEvents(streamState) {
  const events = [];

  // Close any open text block
  if (streamState.textBlockStarted) {
    let needsClose = true;
    // Check if already closed (tool blocks started would have closed it)
    if (Object.keys(streamState.toolBlocksStarted).length > 0) {
      needsClose = false; // Already closed when tool block started
    }
    if (needsClose) {
      events.push({
        type: 'content_block_stop',
        index: 0
      });
    }
  }

  // Close any open tool blocks
  const toolIndices = Object.keys(streamState.toolBlocksStarted).map(Number).sort((a, b) => a - b);
  for (const idx of toolIndices) {
    const toolIndex = streamState.textBlockStarted ? idx + 1 : idx;
    events.push({
      type: 'content_block_stop',
      index: toolIndex
    });
  }

  // Determine stop_reason
  let stopReason = 'end_turn';
  if (Object.keys(streamState.toolCalls).length > 0) {
    stopReason = 'tool_use';
  } else if (streamState.finishReason === 'length') {
    stopReason = 'max_tokens';
  } else if (streamState.finishReason === 'tool_calls') {
    stopReason = 'tool_use';
  }

  // Send message_delta with stop_reason
  events.push({
    type: 'message_delta',
    delta: { stop_reason: stopReason, stop_sequence: null },
    usage: { output_tokens: 0 }
  });

  // Send message_stop
  events.push({
    type: 'message_stop'
  });

  return events;
}

// ============================================================================
// RETRY AND FALLBACK LOGIC
// ============================================================================

// Helper for delay
function delay(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// Make a single request to OpenRouter (returns Promise)
function makeOpenRouterRequest(openaiBody, authToken) {
  return new Promise((resolve, reject) => {
    const bodyStr = JSON.stringify(openaiBody);

    const options = {
      hostname: "openrouter.ai",
      port: 443,
      path: "/api/v1/chat/completions",
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": authToken,
        "User-Agent": "Claude-Proxy/2.1.0",
        "HTTP-Referer": "https://github.com/anthropics/claude-code",
        "X-Title": "Claude Code Proxy"
      }
    };

    const req = https.request(options, (proxyRes) => {
      // Rate limit detection
      if (proxyRes.statusCode === 429) {
        let errorData = '';
        proxyRes.on('data', chunk => errorData += chunk);
        proxyRes.on('end', () => {
          const retryAfter = proxyRes.headers['retry-after'];
          reject({
            type: 'rate_limit',
            statusCode: 429,
            retryAfter: retryAfter ? parseInt(retryAfter, 10) * 1000 : null,
            message: errorData
          });
        });
        return;
      }

      // Other errors
      if (proxyRes.statusCode >= 400) {
        let errorData = '';
        proxyRes.on('data', chunk => errorData += chunk);
        proxyRes.on('end', () => {
          reject({
            type: 'api_error',
            statusCode: proxyRes.statusCode,
            message: errorData
          });
        });
        return;
      }

      // Success - resolve with response stream
      resolve({
        statusCode: proxyRes.statusCode,
        headers: proxyRes.headers,
        stream: proxyRes
      });
    });

    req.on('error', (err) => {
      reject({
        type: 'network_error',
        message: err.message
      });
    });

    req.write(bodyStr);
    req.end();
  });
}

// Make request with retry logic and model fallback
async function makeRequestWithRetry(openaiBody, authToken, originalModel) {
  // Build list of models to try
  const models = [originalModel];
  if (FALLBACK_MODEL && FALLBACK_MODEL !== originalModel) {
    models.push(FALLBACK_MODEL);
  }

  let lastError = null;

  for (let modelIndex = 0; modelIndex < models.length; modelIndex++) {
    const model = models[modelIndex];
    const isLastModel = modelIndex === models.length - 1;
    const bodyWithModel = { ...openaiBody, model };

    for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
      const isLastAttempt = attempt === MAX_RETRIES;

      try {
        console.log(`Trying model: ${model} (attempt ${attempt}/${MAX_RETRIES})`);
        const result = await makeOpenRouterRequest(bodyWithModel, authToken);
        console.log(`Success with model: ${model}`);
        return { ...result, modelUsed: model };

      } catch (err) {
        lastError = err;
        console.log(`Attempt ${attempt}/${MAX_RETRIES} failed for ${model}: ${err.type} - ${err.message?.substring(0, 100) || ''}`);

        // Rate limit with immediate fallback option
        if (err.type === 'rate_limit' && FALLBACK_ON_RATE_LIMIT && !isLastModel) {
          console.log(`Rate limited (429), switching to fallback model: ${FALLBACK_MODEL}`);
          break; // Exit retry loop, try next model
        }

        // If last attempt for this model
        if (isLastAttempt) {
          if (!isLastModel) {
            console.log(`Max retries reached for ${model}, trying fallback: ${models[modelIndex + 1]}`);
          }
          break; // Try next model
        }

        // Calculate delay with exponential backoff
        const backoffDelay = RETRY_DELAY_MS * Math.pow(2, attempt - 1);
        const waitTime = err.retryAfter || backoffDelay;
        console.log(`Waiting ${waitTime}ms before retry...`);
        await delay(waitTime);
      }
    }
  }

  // All models and retries exhausted
  console.log(`All retry attempts exhausted. Last error: ${lastError?.type}`);
  throw lastError;
}

// ============================================================================
// HTTP SERVER AND REQUEST HANDLING
// ============================================================================

// Logging middleware
app.use((req, res, next) => {
  console.log(`[${new Date().toISOString()}] ${req.method} ${req.path}`);
  next();
});

// Proxy endpoint for messages (with retry and fallback support)
app.all("/v1/messages", (req, res) => {
  console.log("Received Anthropic API request");

  if (!req.get("authorization")) {
    return res.status(401).json({ error: "Unauthorized: Missing authorization header" });
  }

  const authToken = req.get("authorization");

  // Log agent/MCP request details
  const hasSystem = !!req.body.system;
  const hasTools = req.body.tools && req.body.tools.length > 0;
  const messageCount = req.body.messages ? req.body.messages.length : 0;
  const hasToolChoice = !!req.body.tool_choice;

  console.log(`Request details: system=${hasSystem}, tools=${req.body.tools?.length || 0}, ` +
              `messages=${messageCount}, tool_choice=${hasToolChoice}, stream=${req.body.stream}`);

  // Log tool names if present
  if (hasTools) {
    const toolNames = req.body.tools.map(t => t.name).join(', ');
    console.log(`  Tools: ${toolNames}`);
  }

  // Convert Anthropic format to OpenAI format
  const openaiBody = anthropicToOpenAI(req.body);
  const originalModel = openaiBody.model;
  const isStreaming = openaiBody.stream;
  console.log(`Converted to OpenAI format, primary model: ${originalModel}, fallback: ${FALLBACK_MODEL}`);

  // Use async IIFE to handle the request with retry logic
  (async () => {
    try {
      // Make request with retry and fallback
      const result = await makeRequestWithRetry(openaiBody, authToken, originalModel);
      const { stream: proxyRes, modelUsed } = result;

      console.log(`OpenRouter response received (model used: ${modelUsed})`);

      // Record request statistics
      recordRequest(modelUsed, isStreaming, hasTools);
      if (modelUsed !== originalModel) {
        recordFallback();
      }

      // For streaming responses
      if (openaiBody.stream) {
        res.writeHead(200, {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
          'X-Accel-Buffering': 'no',
          'X-Model-Used': modelUsed
        });

        const streamState = createStreamState(modelUsed);
        let buffer = '';

        proxyRes.on('data', (chunk) => {
          buffer += chunk.toString();
          const lines = buffer.split('\n');
          buffer = lines.pop() || '';

          for (const line of lines) {
            const trimmedLine = line.trim();
            if (!trimmedLine) continue;

            if (trimmedLine.startsWith('data: ')) {
              const events = processStreamingChunk(trimmedLine, streamState);
              for (const event of events) {
                res.write(`event: ${event.type}\ndata: ${JSON.stringify(event)}\n\n`);
              }
            }
          }
        });

        proxyRes.on('end', () => {
          if (buffer.trim()) {
            const events = processStreamingChunk(buffer.trim(), streamState);
            for (const event of events) {
              res.write(`event: ${event.type}\ndata: ${JSON.stringify(event)}\n\n`);
            }
          }

          const endEvents = generateStreamEndEvents(streamState);
          for (const event of endEvents) {
            res.write(`event: ${event.type}\ndata: ${JSON.stringify(event)}\n\n`);
          }

          console.log(`Streaming complete (model: ${modelUsed}), tool_calls: ${Object.keys(streamState.toolCalls).length}`);
          res.end();
        });

        proxyRes.on('error', (err) => {
          console.error("Streaming error:", err.message);
          res.end();
        });

      } else {
        // Non-streaming response
        let data = '';
        proxyRes.on('data', (chunk) => {
          data += chunk;
        });

        proxyRes.on('end', () => {
          try {
            const openaiResponse = JSON.parse(data);

            if (openaiResponse.error) {
              console.log("Error in response body:", openaiResponse.error);
              res.status(400).json({
                type: 'error',
                error: {
                  type: openaiResponse.error.type || 'api_error',
                  message: openaiResponse.error.message || JSON.stringify(openaiResponse.error)
                }
              });
              return;
            }

            // Convert OpenAI format to Anthropic format
            const anthropicResponse = openAIResponseToAnthropic(openaiResponse, modelUsed);

            // Record token usage
            if (anthropicResponse.usage) {
              recordTokens(modelUsed, anthropicResponse.usage.input_tokens, anthropicResponse.usage.output_tokens);
            }

            // Log response details
            const toolUseCount = anthropicResponse.content.filter(c => c.type === 'tool_use').length;
            console.log(`Response (model: ${modelUsed}): stop_reason=${anthropicResponse.stop_reason}, ` +
                        `content_blocks=${anthropicResponse.content.length}, tool_use=${toolUseCount}`);

            res.setHeader('X-Model-Used', modelUsed);
            res.status(200).json(anthropicResponse);
          } catch (e) {
            console.error("Error parsing response:", e.message);
            console.error("Raw response:", data.substring(0, 500));
            res.status(500).json({
              type: 'error',
              error: { type: 'api_error', message: 'Failed to parse OpenRouter response' }
            });
          }
        });
      }

    } catch (err) {
      // All retries and fallback exhausted
      console.error(`Request failed after all retries: ${err.type} - ${err.message?.substring(0, 200) || ''}`);

      // Record error statistics
      recordError(err.type);

      const statusCode = err.statusCode || 502;
      let errorMessage = 'Request failed after all retry attempts';

      if (err.type === 'rate_limit') {
        errorMessage = 'Rate limited by OpenRouter. All retry attempts exhausted.';
      } else if (err.type === 'network_error') {
        errorMessage = 'Network error connecting to OpenRouter: ' + (err.message || 'Unknown error');
      } else if (err.type === 'api_error') {
        try {
          const parsed = JSON.parse(err.message);
          errorMessage = parsed.error?.message || err.message;
        } catch {
          errorMessage = err.message || 'API error';
        }
      }

      res.status(statusCode).json({
        type: 'error',
        error: {
          type: err.type || 'api_error',
          message: errorMessage
        }
      });
    }
  })();
});

// Health check endpoint
app.get("/health", (req, res) => {
  res.json({
    status: "ok",
    version: "2.2.0",
    features: ["tools", "streaming", "tool_choice", "mcp", "retry", "fallback", "dashboard"]
  });
});

// ============================================================================
// USAGE DASHBOARD
// ============================================================================

function generateHtmlDashboard(statsData) {
  const modelRows = Object.entries(statsData.models)
    .map(([name, data]) => `
      <tr>
        <td>${name}</td>
        <td>${data.requests}</td>
        <td>${data.inputTokens.toLocaleString()}</td>
        <td>${data.outputTokens.toLocaleString()}</td>
      </tr>
    `).join('');

  return `<!DOCTYPE html>
<html>
<head>
  <title>OpenRouter Proxy Dashboard</title>
  <meta http-equiv="refresh" content="10">
  <style>
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
           max-width: 900px; margin: 40px auto; padding: 20px; background: #f5f5f5; }
    h1 { color: #333; }
    .card { background: white; border-radius: 8px; padding: 20px; margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
    .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 20px; }
    .stat { text-align: center; }
    .stat-value { font-size: 2em; font-weight: bold; color: #2563eb; }
    .stat-label { color: #666; font-size: 0.9em; }
    table { width: 100%; border-collapse: collapse; }
    th, td { padding: 10px; text-align: left; border-bottom: 1px solid #eee; }
    th { background: #f8f9fa; }
    .error { color: #dc2626; }
    .success { color: #16a34a; }
    .uptime { color: #666; font-size: 0.9em; }
  </style>
</head>
<body>
  <h1>OpenRouter Proxy Dashboard</h1>
  <p class="uptime">Uptime: ${statsData.uptime} | Last request: ${statsData.lastRequest || 'None'}</p>

  <div class="card">
    <h2>Requests</h2>
    <div class="stats">
      <div class="stat"><div class="stat-value">${statsData.requests.total}</div><div class="stat-label">Total</div></div>
      <div class="stat"><div class="stat-value">${statsData.requests.streaming}</div><div class="stat-label">Streaming</div></div>
      <div class="stat"><div class="stat-value">${statsData.requests.withTools}</div><div class="stat-label">With Tools</div></div>
      <div class="stat"><div class="stat-value">${statsData.fallbacks}</div><div class="stat-label">Fallbacks</div></div>
    </div>
  </div>

  <div class="card">
    <h2>Tokens</h2>
    <div class="stats">
      <div class="stat"><div class="stat-value">${statsData.tokens.total.toLocaleString()}</div><div class="stat-label">Total</div></div>
      <div class="stat"><div class="stat-value">${statsData.tokens.input.toLocaleString()}</div><div class="stat-label">Input</div></div>
      <div class="stat"><div class="stat-value">${statsData.tokens.output.toLocaleString()}</div><div class="stat-label">Output</div></div>
    </div>
  </div>

  <div class="card">
    <h2>Models</h2>
    <table>
      <tr><th>Model</th><th>Requests</th><th>Input Tokens</th><th>Output Tokens</th></tr>
      ${modelRows || '<tr><td colspan="4">No requests yet</td></tr>'}
    </table>
  </div>

  <div class="card">
    <h2>Errors</h2>
    <div class="stats">
      <div class="stat"><div class="stat-value ${statsData.errors.total > 0 ? 'error' : 'success'}">${statsData.errors.total}</div><div class="stat-label">Total</div></div>
      <div class="stat"><div class="stat-value">${statsData.errors.rateLimits}</div><div class="stat-label">Rate Limits</div></div>
      <div class="stat"><div class="stat-value">${statsData.errors.apiErrors}</div><div class="stat-label">API Errors</div></div>
      <div class="stat"><div class="stat-value">${statsData.errors.rate}</div><div class="stat-label">Error Rate</div></div>
    </div>
  </div>

  <p style="text-align:center;color:#999;font-size:0.8em;">Auto-refreshes every 10 seconds</p>
</body>
</html>`;
}

// Dashboard endpoint - returns usage statistics
app.get("/dashboard", (req, res) => {
  const uptime = Date.now() - stats.startTime;
  const statsData = {
    status: "ok",
    uptime: formatUptime(uptime),
    uptimeMs: uptime,
    lastRequest: stats.lastRequest ? new Date(stats.lastRequest).toISOString() : null,
    requests: stats.requests,
    tokens: {
      total: stats.tokens.input + stats.tokens.output,
      input: stats.tokens.input,
      output: stats.tokens.output
    },
    models: stats.models,
    errors: {
      ...stats.errors,
      rate: stats.requests.total > 0
        ? (stats.errors.total / stats.requests.total * 100).toFixed(2) + '%'
        : '0%'
    },
    fallbacks: stats.fallbacks
  };

  if (req.query.format === 'html') {
    res.setHeader('Content-Type', 'text/html');
    res.send(generateHtmlDashboard(statsData));
  } else {
    res.json(statsData);
  }
});

// Info endpoint
app.get("/", (req, res) => {
  res.json({
    name: "OpenRouter Proxy",
    version: "2.2.0",
    status: "running",
    target: OPENROUTER_BASE,
    features: {
      tools: true,
      streaming: true,
      tool_choice: true,
      mcp_support: true,
      retry: true,
      fallback: true,
      dashboard: true
    },
    config: {
      fallback_model: FALLBACK_MODEL,
      max_retries: MAX_RETRIES,
      retry_delay_ms: RETRY_DELAY_MS,
      fallback_on_rate_limit: FALLBACK_ON_RATE_LIMIT
    },
    endpoints: {
      messages: "/v1/messages",
      health: "/health",
      dashboard: "/dashboard",
      dashboard_html: "/dashboard?format=html"
    }
  });
});

// Debug endpoint to test tool conversion (useful for troubleshooting)
app.post("/debug/convert", (req, res) => {
  try {
    const converted = anthropicToOpenAI(req.body);
    res.json({
      original: req.body,
      converted: converted
    });
  } catch (e) {
    res.status(400).json({ error: e.message });
  }
});

// Start server
app.listen(PORT, () => {
  console.log("");
  console.log("╔════════════════════════════════════════════════════════════╗");
  console.log("║  OpenRouter Proxy v2.2.0 - Usage Dashboard                 ║");
  console.log("╠════════════════════════════════════════════════════════════╣");
  console.log("║  Local:     http://localhost:" + PORT + "                         ║");
  console.log("║  Target:    " + OPENROUTER_BASE + "              ║");
  console.log("║  Dashboard: http://localhost:" + PORT + "/dashboard               ║");
  console.log("║  Features:  tools, streaming, mcp, retry, fallback         ║");
  console.log("╠════════════════════════════════════════════════════════════╣");
  console.log("║  Retry Config:                                             ║");
  console.log(`║    Fallback Model: ${FALLBACK_MODEL.padEnd(39)}║`);
  console.log(`║    Max Retries:    ${String(MAX_RETRIES).padEnd(39)}║`);
  console.log(`║    Retry Delay:    ${(RETRY_DELAY_MS + 'ms').padEnd(39)}║`);
  console.log(`║    Fallback on 429: ${String(FALLBACK_ON_RATE_LIMIT).padEnd(38)}║`);
  console.log("╚════════════════════════════════════════════════════════════╝");
  console.log("");
});

app.on("error", (err) => {
  console.error("Server error:", err);
  process.exit(1);
});
PROXYEOF

# Expose port
EXPOSE 8787

# Health check
HEALTHCHECK --interval=10s --timeout=5s --retries=5 --start-period=10s \
    CMD curl -f http://localhost:8787/health || exit 1

# Start proxy
CMD ["npm", "start"]
