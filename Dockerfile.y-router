# syntax=docker/dockerfile:1
# Standalone Node.js proxy for Anthropic to OpenAI format conversion
# Properly handles Claude Code requests and forwards to OpenRouter

FROM node:22-bookworm-slim

# Install curl for health checks
RUN apt-get update && apt-get install -y --no-install-recommends curl && rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Create package.json
RUN cat > package.json << 'EOF'
{
  "name": "openrouter-proxy",
  "version": "1.0.0",
  "description": "Anthropic to OpenRouter proxy",
  "main": "proxy.js",
  "scripts": {
    "start": "node proxy.js"
  },
  "dependencies": {
    "express": "^4.18.2"
  }
}
EOF

# Install dependencies
RUN npm install

# Create proxy server that converts Anthropic API to OpenAI format
RUN cat > proxy.js << 'PROXYEOF'
const express = require("express");
const https = require("https");

const app = express();
app.use(express.json({ limit: "50mb" }));

const PORT = 8787;
const OPENROUTER_BASE = "https://openrouter.ai/api/v1";

// ============================================================================
// FORMAT CONVERSION FUNCTIONS
// ============================================================================

// Convert Anthropic tool_choice to OpenAI format
// Anthropic: { type: "auto" | "any" | "tool", name?: string }
// OpenAI: "auto" | "required" | { type: "function", function: { name: string } }
function convertToolChoice(anthropicChoice) {
  if (!anthropicChoice) return undefined;

  if (anthropicChoice.type === 'auto') return 'auto';
  if (anthropicChoice.type === 'any') return 'required';
  if (anthropicChoice.type === 'tool' && anthropicChoice.name) {
    return { type: 'function', function: { name: anthropicChoice.name } };
  }

  // Pass through if already in OpenAI format or unknown
  return anthropicChoice;
}

// Convert Anthropic tools to OpenAI functions
function convertTools(anthropicTools) {
  if (!anthropicTools || !Array.isArray(anthropicTools)) return undefined;

  return anthropicTools.map(tool => ({
    type: 'function',
    function: {
      name: tool.name,
      description: tool.description || '',
      parameters: tool.input_schema || { type: 'object', properties: {} }
    }
  }));
}

// Convert Anthropic messages to OpenAI format (with agent/MCP support)
// Handles: text, tool_use (in assistant messages), tool_result (in user messages)
function convertMessages(anthropicMessages) {
  const openaiMessages = [];

  for (const msg of anthropicMessages) {
    // Simple string content
    if (typeof msg.content === 'string') {
      openaiMessages.push({
        role: msg.role,
        content: msg.content
      });
      continue;
    }

    // Array content - needs special handling
    if (Array.isArray(msg.content)) {
      if (msg.role === 'assistant') {
        // Assistant messages may contain tool_use blocks
        const textParts = [];
        const toolCalls = [];

        for (const block of msg.content) {
          if (block.type === 'text') {
            textParts.push(block.text);
          } else if (block.type === 'tool_use') {
            // Convert Anthropic tool_use to OpenAI tool_calls format
            toolCalls.push({
              id: block.id,
              type: 'function',
              function: {
                name: block.name,
                arguments: typeof block.input === 'string'
                  ? block.input
                  : JSON.stringify(block.input || {})
              }
            });
          }
        }

        const assistantMsg = {
          role: 'assistant',
          content: textParts.join('\n') || null
        };

        if (toolCalls.length > 0) {
          assistantMsg.tool_calls = toolCalls;
        }

        openaiMessages.push(assistantMsg);
      } else if (msg.role === 'user') {
        // User messages may contain tool_result blocks
        // These need to be split into separate "tool" role messages
        const textParts = [];
        const toolResults = [];

        for (const block of msg.content) {
          if (block.type === 'text') {
            textParts.push(block.text);
          } else if (block.type === 'tool_result') {
            toolResults.push(block);
          } else if (block.type === 'image') {
            // Handle image blocks (vision)
            textParts.push({
              type: 'image_url',
              image_url: {
                url: block.source?.data
                  ? `data:${block.source.media_type};base64,${block.source.data}`
                  : block.source?.url || ''
              }
            });
          } else {
            // Pass through other content types
            textParts.push(block);
          }
        }

        // Add tool results as separate "tool" role messages (OpenAI format)
        for (const tr of toolResults) {
          let content = tr.content;

          // Handle array content in tool results
          if (Array.isArray(content)) {
            content = content.map(c => {
              if (typeof c === 'string') return c;
              if (c.type === 'text') return c.text;
              return JSON.stringify(c);
            }).join('\n');
          } else if (typeof content !== 'string') {
            content = JSON.stringify(content);
          }

          openaiMessages.push({
            role: 'tool',
            tool_call_id: tr.tool_use_id,
            content: content || ''
          });
        }

        // Add text content as user message if present
        if (textParts.length > 0) {
          const hasComplexContent = textParts.some(p => typeof p !== 'string');
          openaiMessages.push({
            role: 'user',
            content: hasComplexContent ? textParts : textParts.join('\n')
          });
        }
      } else {
        // Other roles - pass through
        openaiMessages.push({
          role: msg.role,
          content: msg.content
        });
      }
    } else {
      // Fallback - pass through as-is
      openaiMessages.push({
        role: msg.role,
        content: msg.content
      });
    }
  }

  return openaiMessages;
}

// Main request converter: Anthropic -> OpenAI
function anthropicToOpenAI(anthropicMsg) {
  let messages = convertMessages(anthropicMsg.messages || []);

  // Handle system prompt - insert as first message
  if (anthropicMsg.system) {
    const systemContent = typeof anthropicMsg.system === 'string'
      ? anthropicMsg.system
      : Array.isArray(anthropicMsg.system)
        ? anthropicMsg.system.map(s => s.text || s).join('\n')
        : anthropicMsg.system;

    messages.unshift({
      role: 'system',
      content: systemContent
    });
  }

  const openaiMsg = {
    model: anthropicMsg.model,
    messages: messages,
    max_tokens: anthropicMsg.max_tokens,
    temperature: anthropicMsg.temperature,
    top_p: anthropicMsg.top_p,
    stream: anthropicMsg.stream === true
  };

  // Add tools if present
  if (anthropicMsg.tools && anthropicMsg.tools.length > 0) {
    openaiMsg.tools = convertTools(anthropicMsg.tools);
  }

  // Convert tool_choice
  if (anthropicMsg.tool_choice) {
    openaiMsg.tool_choice = convertToolChoice(anthropicMsg.tool_choice);
  }

  // Convert stop_sequences to stop
  if (anthropicMsg.stop_sequences) {
    openaiMsg.stop = anthropicMsg.stop_sequences;
  }

  // Remove undefined values
  Object.keys(openaiMsg).forEach(key => openaiMsg[key] === undefined && delete openaiMsg[key]);

  console.log("Converted request - tools:", openaiMsg.tools ? openaiMsg.tools.length : 0,
              "tool_choice:", openaiMsg.tool_choice,
              "messages:", openaiMsg.messages.length,
              "stream:", openaiMsg.stream);
  return openaiMsg;
}

// ============================================================================
// NON-STREAMING RESPONSE CONVERSION
// ============================================================================

// Convert OpenAI complete response to Anthropic format
function openAIResponseToAnthropic(openaiResponse, model) {
  if (!openaiResponse.choices || openaiResponse.choices.length === 0) {
    return {
      id: openaiResponse.id || 'msg_' + Date.now(),
      type: 'message',
      role: 'assistant',
      content: [{ type: 'text', text: '' }],
      model: model,
      stop_reason: 'end_turn',
      usage: {
        input_tokens: openaiResponse.usage?.prompt_tokens || 0,
        output_tokens: openaiResponse.usage?.completion_tokens || 0
      }
    };
  }

  const choice = openaiResponse.choices[0];
  const message = choice.message || {};

  // Build content array
  const content = [];

  // Add text content if present
  if (message.content && typeof message.content === 'string' && message.content.trim()) {
    content.push({
      type: 'text',
      text: message.content
    });
  }

  // Convert tool calls to Anthropic tool_use format
  if (message.tool_calls && Array.isArray(message.tool_calls)) {
    console.log("Converting", message.tool_calls.length, "tool calls to Anthropic format");

    for (const toolCall of message.tool_calls) {
      if (toolCall.type === 'function') {
        let parsedInput = {};

        if (typeof toolCall.function.arguments === 'string') {
          try {
            parsedInput = JSON.parse(toolCall.function.arguments);
          } catch (e) {
            console.error("Failed to parse tool arguments:", e.message);
            // Try to salvage what we can
            parsedInput = { _raw: toolCall.function.arguments };
          }
        } else {
          parsedInput = toolCall.function.arguments || {};
        }

        content.push({
          type: 'tool_use',
          id: toolCall.id,
          name: toolCall.function.name,
          input: parsedInput
        });
      }
    }
  }

  // Ensure at least empty text if no content
  if (content.length === 0) {
    content.push({
      type: 'text',
      text: ''
    });
  }

  // Determine stop_reason
  let stopReason = 'end_turn';
  if (message.tool_calls && message.tool_calls.length > 0) {
    stopReason = 'tool_use';
  } else if (choice.finish_reason === 'length') {
    stopReason = 'max_tokens';
  } else if (choice.finish_reason === 'stop') {
    stopReason = 'end_turn';
  } else if (choice.finish_reason === 'tool_calls') {
    stopReason = 'tool_use';
  }

  return {
    id: openaiResponse.id || 'msg_' + Date.now(),
    type: 'message',
    role: 'assistant',
    content: content,
    model: model,
    stop_reason: stopReason,
    usage: {
      input_tokens: openaiResponse.usage?.prompt_tokens || 0,
      output_tokens: openaiResponse.usage?.completion_tokens || 0
    }
  };
}

// ============================================================================
// STREAMING RESPONSE CONVERSION
// ============================================================================

// Create a new stream state for tracking streaming responses
function createStreamState(model) {
  return {
    model: model,
    messageId: 'msg_' + Date.now(),
    started: false,           // Has message_start been sent?
    textBlockStarted: false,  // Has text content_block_start been sent?
    textContent: '',          // Accumulated text content
    toolCalls: {},            // Track tool calls by index: { id, name, args }
    toolBlocksStarted: {},    // Track which tool blocks have been started
    contentIndex: 0,          // Current content block index
    finishReason: null        // Final finish reason
  };
}

// Process a streaming chunk and return Anthropic SSE events
function processStreamingChunk(line, streamState) {
  const events = [];

  // Parse SSE data line
  if (!line.startsWith('data: ')) {
    return events;
  }

  const jsonStr = line.slice(6).trim();
  if (jsonStr === '[DONE]') {
    return events; // Will be handled in stream end
  }

  let data;
  try {
    data = JSON.parse(jsonStr);
  } catch (e) {
    console.error('Failed to parse streaming chunk:', e.message);
    return events;
  }

  if (!data.choices || !data.choices[0]) {
    return events;
  }

  const choice = data.choices[0];
  const delta = choice.delta || {};

  // Emit message_start on first chunk
  if (!streamState.started) {
    streamState.started = true;
    events.push({
      type: 'message_start',
      message: {
        id: streamState.messageId,
        type: 'message',
        role: 'assistant',
        content: [],
        model: streamState.model,
        stop_reason: null,
        stop_sequence: null,
        usage: { input_tokens: 0, output_tokens: 0 }
      }
    });
  }

  // Handle text content delta
  if (delta.content) {
    // Start text block if not started
    if (!streamState.textBlockStarted) {
      streamState.textBlockStarted = true;
      streamState.contentIndex = 0;
      events.push({
        type: 'content_block_start',
        index: 0,
        content_block: { type: 'text', text: '' }
      });
    }

    // Send text delta
    streamState.textContent += delta.content;
    events.push({
      type: 'content_block_delta',
      index: 0,
      delta: { type: 'text_delta', text: delta.content }
    });
  }

  // Handle tool calls delta
  if (delta.tool_calls && delta.tool_calls.length > 0) {
    for (const tc of delta.tool_calls) {
      const idx = tc.index;

      // Initialize tool call tracking if new
      if (!streamState.toolCalls[idx]) {
        streamState.toolCalls[idx] = { id: '', name: '', args: '' };
      }

      const toolState = streamState.toolCalls[idx];

      // Capture tool call id
      if (tc.id) {
        toolState.id = tc.id;
      }

      // Capture/accumulate function name
      if (tc.function?.name) {
        toolState.name += tc.function.name;
      }

      // Accumulate function arguments
      if (tc.function?.arguments) {
        toolState.args += tc.function.arguments;
      }

      // Start tool_use block when we have id and name
      if (toolState.id && toolState.name && !streamState.toolBlocksStarted[idx]) {
        // Close text block first if it was started
        if (streamState.textBlockStarted && streamState.contentIndex === 0) {
          events.push({
            type: 'content_block_stop',
            index: 0
          });
          streamState.contentIndex = 1;
        } else if (!streamState.textBlockStarted) {
          // No text block was started, tool starts at index 0
          streamState.contentIndex = 0;
        }

        streamState.toolBlocksStarted[idx] = true;
        const toolIndex = streamState.textBlockStarted ? idx + 1 : idx;

        events.push({
          type: 'content_block_start',
          index: toolIndex,
          content_block: {
            type: 'tool_use',
            id: toolState.id,
            name: toolState.name,
            input: {}
          }
        });
      }

      // Send input_json_delta for arguments
      if (tc.function?.arguments && streamState.toolBlocksStarted[idx]) {
        const toolIndex = streamState.textBlockStarted ? idx + 1 : idx;
        events.push({
          type: 'content_block_delta',
          index: toolIndex,
          delta: {
            type: 'input_json_delta',
            partial_json: tc.function.arguments
          }
        });
      }
    }
  }

  // Handle finish reason
  if (choice.finish_reason) {
    streamState.finishReason = choice.finish_reason;
  }

  return events;
}

// Generate final events when stream ends
function generateStreamEndEvents(streamState) {
  const events = [];

  // Close any open text block
  if (streamState.textBlockStarted) {
    let needsClose = true;
    // Check if already closed (tool blocks started would have closed it)
    if (Object.keys(streamState.toolBlocksStarted).length > 0) {
      needsClose = false; // Already closed when tool block started
    }
    if (needsClose) {
      events.push({
        type: 'content_block_stop',
        index: 0
      });
    }
  }

  // Close any open tool blocks
  const toolIndices = Object.keys(streamState.toolBlocksStarted).map(Number).sort((a, b) => a - b);
  for (const idx of toolIndices) {
    const toolIndex = streamState.textBlockStarted ? idx + 1 : idx;
    events.push({
      type: 'content_block_stop',
      index: toolIndex
    });
  }

  // Determine stop_reason
  let stopReason = 'end_turn';
  if (Object.keys(streamState.toolCalls).length > 0) {
    stopReason = 'tool_use';
  } else if (streamState.finishReason === 'length') {
    stopReason = 'max_tokens';
  } else if (streamState.finishReason === 'tool_calls') {
    stopReason = 'tool_use';
  }

  // Send message_delta with stop_reason
  events.push({
    type: 'message_delta',
    delta: { stop_reason: stopReason, stop_sequence: null },
    usage: { output_tokens: 0 }
  });

  // Send message_stop
  events.push({
    type: 'message_stop'
  });

  return events;
}

// ============================================================================
// HTTP SERVER AND REQUEST HANDLING
// ============================================================================

// Logging middleware
app.use((req, res, next) => {
  console.log(`[${new Date().toISOString()}] ${req.method} ${req.path}`);
  next();
});

// Proxy endpoint for messages
app.all("/v1/messages", (req, res) => {
  console.log("Received Anthropic API request");

  if (!req.get("authorization")) {
    return res.status(401).json({ error: "Unauthorized: Missing authorization header" });
  }

  const authToken = req.get("authorization");

  // Log agent/MCP request details
  const hasSystem = !!req.body.system;
  const hasTools = req.body.tools && req.body.tools.length > 0;
  const messageCount = req.body.messages ? req.body.messages.length : 0;
  const hasToolChoice = !!req.body.tool_choice;

  console.log(`Request details: system=${hasSystem}, tools=${req.body.tools?.length || 0}, ` +
              `messages=${messageCount}, tool_choice=${hasToolChoice}, stream=${req.body.stream}`);

  // Log tool names if present
  if (hasTools) {
    const toolNames = req.body.tools.map(t => t.name).join(', ');
    console.log(`  Tools: ${toolNames}`);
  }

  // Convert Anthropic format to OpenAI format
  const openaiBody = anthropicToOpenAI(req.body);
  console.log("Converted to OpenAI format, sending to OpenRouter");

  // Prepare OpenRouter request
  const options = {
    hostname: "openrouter.ai",
    port: 443,
    path: "/api/v1/chat/completions",
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Authorization": authToken,
      "User-Agent": "Claude-Proxy/1.0",
      "HTTP-Referer": "https://github.com/anthropics/claude-code",
      "X-Title": "Claude Code Proxy"
    }
  };

  const bodyStr = JSON.stringify(openaiBody);

  const proxyReq = https.request(options, (proxyRes) => {
    console.log("OpenRouter response status:", proxyRes.statusCode);

    // Handle error status codes
    if (proxyRes.statusCode >= 400) {
      let errorData = '';
      proxyRes.on('data', chunk => errorData += chunk);
      proxyRes.on('end', () => {
        console.error("OpenRouter error response:", errorData);
        try {
          const errorJson = JSON.parse(errorData);
          // Convert to Anthropic error format
          res.status(proxyRes.statusCode).json({
            type: 'error',
            error: {
              type: errorJson.error?.type || 'api_error',
              message: errorJson.error?.message || errorData
            }
          });
        } catch (e) {
          res.status(proxyRes.statusCode).json({
            type: 'error',
            error: { type: 'api_error', message: errorData }
          });
        }
      });
      return;
    }

    // For streaming responses
    if (openaiBody.stream) {
      res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'X-Accel-Buffering': 'no'
      });

      const streamState = createStreamState(openaiBody.model);
      let buffer = '';

      proxyRes.on('data', (chunk) => {
        buffer += chunk.toString();
        const lines = buffer.split('\n');
        buffer = lines.pop() || ''; // Keep incomplete line in buffer

        for (const line of lines) {
          const trimmedLine = line.trim();
          if (!trimmedLine) continue;

          if (trimmedLine.startsWith('data: ')) {
            const events = processStreamingChunk(trimmedLine, streamState);
            for (const event of events) {
              res.write(`event: ${event.type}\ndata: ${JSON.stringify(event)}\n\n`);
            }
          }
        }
      });

      proxyRes.on('end', () => {
        // Process any remaining buffer
        if (buffer.trim()) {
          const events = processStreamingChunk(buffer.trim(), streamState);
          for (const event of events) {
            res.write(`event: ${event.type}\ndata: ${JSON.stringify(event)}\n\n`);
          }
        }

        // Generate end events
        const endEvents = generateStreamEndEvents(streamState);
        for (const event of endEvents) {
          res.write(`event: ${event.type}\ndata: ${JSON.stringify(event)}\n\n`);
        }

        console.log("Streaming complete, tool_calls:", Object.keys(streamState.toolCalls).length);
        res.end();
      });

      proxyRes.on('error', (err) => {
        console.error("Streaming error:", err.message);
        res.end();
      });
    } else {
      // Non-streaming response
      let data = '';
      proxyRes.on('data', (chunk) => {
        data += chunk;
      });

      proxyRes.on('end', () => {
        try {
          const openaiResponse = JSON.parse(data);
          console.log("OpenRouter response received");

          // If it's an error response, convert to Anthropic format
          if (openaiResponse.error) {
            console.log("Error response from OpenRouter:", openaiResponse.error);
            res.status(proxyRes.statusCode >= 400 ? proxyRes.statusCode : 400).json({
              type: 'error',
              error: {
                type: openaiResponse.error.type || 'api_error',
                message: openaiResponse.error.message || JSON.stringify(openaiResponse.error)
              }
            });
            return;
          }

          // Convert OpenAI format to Anthropic format
          const anthropicResponse = openAIResponseToAnthropic(openaiResponse, openaiBody.model);

          // Log response details
          const toolUseCount = anthropicResponse.content.filter(c => c.type === 'tool_use').length;
          console.log(`Response: stop_reason=${anthropicResponse.stop_reason}, ` +
                      `content_blocks=${anthropicResponse.content.length}, tool_use=${toolUseCount}`);

          res.status(200).json(anthropicResponse);
        } catch (e) {
          console.error("Error parsing response:", e.message);
          console.error("Raw response:", data.substring(0, 500));
          res.status(500).json({
            type: 'error',
            error: { type: 'api_error', message: 'Failed to parse OpenRouter response' }
          });
        }
      });
    }
  });

  proxyReq.on("error", (err) => {
    console.error("OpenRouter connection error:", err.message);
    res.status(502).json({
      type: 'error',
      error: { type: 'api_error', message: 'Failed to connect to OpenRouter: ' + err.message }
    });
  });

  proxyReq.write(bodyStr);
  proxyReq.end();
});

// Health check endpoint
app.get("/health", (req, res) => {
  res.json({
    status: "ok",
    version: "2.0.0",
    features: ["tools", "streaming", "tool_choice", "mcp"]
  });
});

// Info endpoint
app.get("/", (req, res) => {
  res.json({
    name: "OpenRouter Proxy",
    version: "2.0.0",
    status: "running",
    target: OPENROUTER_BASE,
    features: {
      tools: true,
      streaming: true,
      tool_choice: true,
      mcp_support: true
    }
  });
});

// Debug endpoint to test tool conversion (useful for troubleshooting)
app.post("/debug/convert", (req, res) => {
  try {
    const converted = anthropicToOpenAI(req.body);
    res.json({
      original: req.body,
      converted: converted
    });
  } catch (e) {
    res.status(400).json({ error: e.message });
  }
});

// Start server
app.listen(PORT, () => {
  console.log("");
  console.log("╔════════════════════════════════════════════════════════════╗");
  console.log("║  OpenRouter Proxy v2.0.0 - MCP/Tools Support               ║");
  console.log("╠════════════════════════════════════════════════════════════╣");
  console.log("║  Local:    http://localhost:" + PORT + "                          ║");
  console.log("║  Target:   " + OPENROUTER_BASE + "               ║");
  console.log("║  Features: tools, streaming, tool_choice, mcp              ║");
  console.log("╚════════════════════════════════════════════════════════════╝");
  console.log("");
});

app.on("error", (err) => {
  console.error("Server error:", err);
  process.exit(1);
});
PROXYEOF

# Expose port
EXPOSE 8787

# Health check
HEALTHCHECK --interval=10s --timeout=5s --retries=5 --start-period=10s \
    CMD curl -f http://localhost:8787/health || exit 1

# Start proxy
CMD ["npm", "start"]
